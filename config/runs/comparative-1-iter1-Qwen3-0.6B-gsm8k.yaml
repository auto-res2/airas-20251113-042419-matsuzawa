run_id: comparative-1-iter1-Qwen3-0.6B-gsm8k
method: HiNoALR
model:
  name: Qwen3-0.6B
  architecture: transformer-decoder
  quantisation:
    scheme: int4
    loader: bitsandbytes
  lora:
    enabled: true
    rank: ${optuna:lora_rank}
    alpha: 32
    dropout: 0.05
  init_checkpoint: hf://Qwen/Qwen3-0.6B
dataset:
  name: openai/gsm8k
  config: main
  max_length: 512
  preprocessing:
    shuffle_buffer: 10000
    add_bos_token: true
training:
  epochs: 3
  max_steps: 20000
  batch_size: ${optuna:batch_size}
  gradient_accumulation_steps: 1
  base_learning_rate: ${optuna:base_learning_rate}
  lr_scheduler: cosine
  warmup_steps: 500
  weight_decay: 0.01
  optimizer: adamw
  beta1: 0.9
  beta2: 0.98
  eps: 1.0e-8
  mixed_precision: fp16
  eval_interval_steps: 1000
  save_interval_steps: 1000
hinoalr:
  enabled: true
  trigger_threshold: 0.3
  downscale_factor: 0.5
  observe_window: 32
  single_shot: true
hardware:
  gpus: 1
  gpu_type: A100-80GB
  cpu_ram_gb: 64
optuna:
  n_trials: 50
  direction: maximize
  metric_name: dev_accuracy_auc
  search_space:
    base_learning_rate:
      type: loguniform
      low: 1.0e-4
      high: 3.0e-4
    lora_rank:
      type: categorical
      choices: [8, 16, 32]
    batch_size:
      type: categorical
      choices: [32, 128]
